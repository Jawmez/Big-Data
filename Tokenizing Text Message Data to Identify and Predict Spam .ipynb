{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text Message Data to Identify and Predict Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In this project I looked into using basic machine learning and complex text vectorization to predict whether a text message was spam or not. For the big data aspect of this project, I analyzed a set of data collected by the spam archive. This sight stated that it was a list of text messages that many \"average\" citizens had recieved over the past year. From this data set, I classified the messages, in order to calculate the predicted percent of messages that the average user receives is spam. The purpose of this was to find how often and how frequently companies send advertisments to phone users. \n",
    "\n",
    "I was intrested in this project because text messages are relevant to me (and everyone). I also noticed that many people (including me) complain about reciving \"too many\" spam text messages. This project will determine what the actual percent of messages the average user recieves which is classified as spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We imported the csv sms spam file into an organized list using the pandas import statement. In this list, spam messages are categorized as spam, and non spam messages are categorized as ham. This organization makes splitting the data into different classes (train and labels and test) much easier. This data is from UCI Machine learning. The data set is called SMS Spam Collection Data Set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 472)\t0.271851684175\n",
      "  (0, 4806)\t0.271851684175\n",
      "  (0, 3863)\t0.090064750337\n",
      "  (0, 6363)\t0.133268350836\n",
      "  (0, 5110)\t0.110581844957\n",
      "  (0, 7936)\t0.160644082652\n",
      "  (0, 1425)\t0.182835174112\n",
      "  (0, 2538)\t0.115644909871\n",
      "  (0, 1743)\t0.11141201918\n",
      "  (0, 7245)\t0.146013424599\n",
      "  (0, 478)\t0.271851684175\n",
      "  (0, 1851)\t0.21934953621\n",
      "  (0, 1383)\t0.148716702655\n",
      "  (0, 7682)\t0.15530772968\n",
      "  (0, 8150)\t0.118013534293\n",
      "  (0, 3224)\t0.122300733507\n",
      "  (0, 1970)\t0.271851684175\n",
      "  (0, 7705)\t0.324568003175\n",
      "  (0, 5174)\t0.102426140378\n",
      "  (0, 5819)\t0.250504415332\n",
      "  (0, 8156)\t0.0996053050366\n",
      "  (0, 3137)\t0.0984238824146\n",
      "  (0, 7200)\t0.163248247172\n",
      "  (0, 4447)\t0.460284912592\n",
      "  (1, 1790)\t0.369929821608\n",
      "  :\t:\n",
      "  (8, 3449)\t0.172818514477\n",
      "  (8, 3572)\t0.116564662538\n",
      "  (8, 5176)\t0.190803658853\n",
      "  (8, 7265)\t0.200793180274\n",
      "  (8, 1028)\t0.215127904721\n",
      "  (8, 4780)\t0.223246772394\n",
      "  (8, 1117)\t0.122793433021\n",
      "  (8, 4202)\t0.143928572963\n",
      "  (8, 4332)\t0.181598012047\n",
      "  (8, 4110)\t0.129529092589\n",
      "  (8, 4590)\t0.200793180274\n",
      "  (8, 4285)\t0.306511697412\n",
      "  (8, 7214)\t0.093048352438\n",
      "  (8, 4670)\t0.106813544072\n",
      "  (8, 7336)\t0.0756987571118\n",
      "  (8, 6363)\t0.150259537842\n",
      "  (8, 2538)\t0.130389177941\n",
      "  (8, 1743)\t0.12561661045\n",
      "  (8, 8150)\t0.164924183249\n",
      "  (9, 1718)\t0.389207471195\n",
      "  (9, 3185)\t0.489859818417\n",
      "  (9, 4670)\t0.361084982425\n",
      "  (9, 3324)\t0.435740933055\n",
      "  (9, 7914)\t0.466609254135\n",
      "  (9, 8150)\t0.26566554415\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "from sklearn import cross_validation\n",
    "data_train, data_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.1, \n",
    "    random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "data_train_transformed = vectorizer.fit_transform(data_train)\n",
    "data_test_transformed  = vectorizer.transform(data_test)\n",
    "\n",
    "print (data_train_transformed[:10])\n",
    "\n",
    "\n",
    "# slim the data for training and testing\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(data_train_transformed, labels_train)\n",
    "data_train_transformed = selector.transform(data_train_transformed).toarray()\n",
    "data_test_transformed  = selector.transform(data_test_transformed).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3245    Funny fact Nobody teaches volcanoes 2 erupt, t...\n",
      "944     I sent my scores to sophas and i had to do sec...\n",
      "1044    We know someone who you know that fancies you....\n",
      "2484    Only if you promise your getting out as SOON a...\n",
      "812     Congratulations ur awarded either å£500 of CD ...\n",
      "2973           I'll text carlos and let you know, hang on\n",
      "2991            K.i did't see you.:)k:)where are you now?\n",
      "2942               No message..no responce..what happend?\n",
      "230     Get down in gandhipuram and walk to cross cut ...\n",
      "1181                           You flippin your shit yet?\n",
      "1912    For real tho this sucks. I can't even cook my ...\n",
      "1992    Free tones Hope you enjoyed your new content. ...\n",
      "5435                    I'm wif him now buying tix lar...\n",
      "4805          Call me when u finish then i come n pick u.\n",
      "401               Dear how is chechi. Did you talk to her\n",
      "1859            What's up. Do you want me to come online?\n",
      "1344                     Were somewhere on Fredericksburg\n",
      "2952    URGENT! Your mobile was awarded a å£1,500 Bonu...\n",
      "501                                 When can Ì_ come out?\n",
      "3337              K, if u bored up just come to my home..\n",
      "1945    Can Ì_ call me at 10:10 to make sure dat i've ...\n",
      "3142    Boy; I love u Grl: Hogolo Boy: gold chain kods...\n",
      "2422            A bloo bloo bloo I'll miss the first bowl\n",
      "381     Yeah sure, give me a couple minutes to track d...\n",
      "5567    This is the 2nd time we have tried 2 contact u...\n",
      "4937    No it was cancelled yeah baby! Well that sound...\n",
      "79      Its not the same here. Still looking for a job...\n",
      "5240     Gud gud..k, chikku tke care.. sleep well gud nyt\n",
      "2554                      I'll reach in ard 20 mins ok...\n",
      "5345                                    Wat Ì_ doing now?\n",
      "                              ...                        \n",
      "3288    Camera - You are awarded a SiPix Digital Camer...\n",
      "292                                 Oops. 4 got that bit.\n",
      "3885    Same, I'm at my great aunts anniversary party ...\n",
      "3846    Fantasy Football is back on your TV. Go to Sky...\n",
      "4537     Dare i ask... Any luck with sorting out the car?\n",
      "4074    God picked up a flower and dippeditinaDEW, lov...\n",
      "1491    Your account has been credited with 500 FREE T...\n",
      "555     O. Well uv causes mutations. Sunscreen is like...\n",
      "1095                              Ryder unsold.now gibbs.\n",
      "1255    Just wait till end of march when el nino gets ...\n",
      "5171                         Oh k. . I will come tomorrow\n",
      "5526    PRIVATE! Your 2003 Account Statement for shows...\n",
      "3256    No, but you told me you were going, before you...\n",
      "611     Its a valentine game. . . Send dis msg to all ...\n",
      "994                     I can't, I don't have her number!\n",
      "2053    Oh... I was thkin of goin yogasana at 10 den n...\n",
      "1888    No. On the way home. So if not for the long dr...\n",
      "907     I.ll give her once i have it. Plus she said gr...\n",
      "705     I don't think I can get away for a trek that l...\n",
      "1813                             Yes we are chatting too.\n",
      "2387                               Also where's the piece\n",
      "655         Tell them the drug dealer's getting impatient\n",
      "3755     Yes:)here tv is always available in work place..\n",
      "3014     &lt;#&gt;  mins but i had to stop somewhere f...\n",
      "505     No it's waiting in e car dat's bored wat. Cos ...\n",
      "3586    Our ride equally uneventful - not too many of ...\n",
      "4686                                    Eatin my lunch...\n",
      "4046    Thanks for your ringtone order, reference numb...\n",
      "2836                              1's reach home call me.\n",
      "4913    You've already got a flaky parent. It'snot sup...\n",
      "Name: v2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "data_train_transformed = vectorizer.fit_transform(data_train)\n",
    "data_test_transformed  = vectorizer.transform(data_test)\n",
    "\n",
    "# slim the data for training and testing\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(data_train_transformed, labels_train)\n",
    "data_train_transformed = selector.transform(data_train_transformed).toarray()\n",
    "data_test_transformed  = selector.transform(data_test_transformed).toarray()\n",
    "\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I seperated my data list into different groups. The groups are categorized based on their classification. In order to randomize each list, I ran a randomize function to scramble the order of each list. \n",
    "\n",
    "After creating each seperate list, I used functions from sklearn to begin vectorizing the text. Vectorization is the process of converting text into numbers based on certain parameters. In this case, the text was given a number based on the frequency the given words appeared in spam messages compared to ham messages. Words that were commonly associated with spam messages where classified as spam words. If these words appeared in a message, the program would most likley classify the message as spam. \n",
    "\n",
    "The next step in tokenizing and vectorizing the text is sliming the data. In order to do this, we need to use sklearn to import select percentile. This program coverts each line into a toarray. The purpose of a toarray is to condense and store all values. Without a toarray, the program will attempt to store all of the values in a sparse matrix. A sparse matrix will keep all \"significant numbers.\" This means that all zeros are erased from the matrix. We do not want our zeros to be erased from our collected data. Without zeros, the result of our code would change significantly. After the vectorized text has been proporly fitted, it can be used to predict whether or not a given message is spam. \n",
    "\n",
    "I found help for this code online. The online code gave me an explanation along with examples of how to use sklearn and text vectorization. The online example used a count vectorizer instead of a frequency vectorizer. A count vectorzier gives words / messgages based on how often they are used (it basically assigns words different weights based on their predicted importance). This is a link to the code --> http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97311827957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(data_train_transformed, labels_train)\n",
    "predictions = clf.predict(data_test_transformed)\n",
    "\n",
    "print(accuracy_score(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I finished training my data, I decided to test the accuracy of my predictions based on my trained dataset. In order to do this, I used sklearn and imported GaussianNB, and accuracy score. Gaussian NB uses the Naive Bayes method to analyze the dataset. Naive Bays is a set of supervised learning algorithms used to analyze the trained data. In this case, we are simply fitting the data and predicting whether our transformed data is spam or not. After this we can use an accuracy score to calculate how accurately my program was able to classify data. In this case, my code has an above 97% accuracy. This means that my code can very accurately predict whether a text message is spam or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a text message: It's Freddie from DoSomething! Today we are offering a chance to win a $3,000 scholarship for members who stand against bullying. Wanna learn how? Yes or No\n",
      "This text message is ['spam']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "get_input = input(\"Enter a text message: \");\n",
    "text = [get_input]\n",
    "text_transformed  = vectorizer.transform(text) \n",
    "text_test_transformed  = selector.transform(text_transformed).toarray()\n",
    "\n",
    "prediction = clf.predict(text_test_transformed)\n",
    "print(\"This text message is\", prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, I can use my algorithim to predict whether a message should be classified as spam or not. In the above model, I tried inputing a spam message that I recieved from a collge website. My program was able to correctly classify the message as spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 5574 messages in this dataset.\n",
      "Around 14.549938347718866% of the messages in this dataset are spam.\n"
     ]
    }
   ],
   "source": [
    "sc = open(\"csvfile2.csv\", \"r\", errors = \"ignore\"); \n",
    "\n",
    "data_list = []\n",
    "for aa in sc:\n",
    "    data_list.append(aa); \n",
    "\n",
    "spam_list = []\n",
    "ham_list = []\n",
    "\n",
    "for bb in data_list:\n",
    "    text = [bb]\n",
    "    text_transformed  = vectorizer.transform(text) \n",
    "    text_test_transformed  = selector.transform(text_transformed).toarray()\n",
    "\n",
    "    prediction = clf.predict(text_test_transformed)\n",
    "    \n",
    "    if prediction == 'ham':\n",
    "        ham_list.append(bb)\n",
    "    \n",
    "    elif prediction == 'spam':\n",
    "        spam_list.append(bb)\n",
    "\n",
    "spam_len = (len(spam_list))\n",
    "ham_len = (len(ham_list))\n",
    "print(\"There are a total of\", len(data_list), \"messages in this dataset.\")\n",
    "\n",
    "average_spam = (float(spam_len/ham_len)*100)\n",
    "\n",
    "print(\"Around\", str(average_spam) + \"% of the messages in this dataset are spam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final method, I used spam messages from the spam archive to predict the average number of messages a user recieves that is spam. I did this by categorizing the messages into seperate lists based on their classification. After running this test, I found that around 14% of the messages that a typical user receives is spam. This seemed like a pretty signifcant percent of messages. Due to the capatalism buisness-based lifestyle of America, it would make sense that people would receive a large number of advertisments concerning buying or showing interest in certain products. \n",
    "\n",
    "It would have also been interesting to run this test on other sources such as emails and phone calls. With this information, I could look into where people are receiving the most spam. Are people receving large amounts of spam via mobile, phone call, email, or all three? \n",
    "\n",
    "In conclusion, I used machine learning and text vectroization to create a program which can predict (with a high accuracy) the amount of messages that a user receives which is spam. After analyzing a large group of texts, I concluded that 14% of the messgages that the average phone user recives is spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
